#!/usr/bin/env python3
from __future__ import annotations

import argparse
import collections as cl
import pathlib
import subprocess
import os
from typing import Dict, List, Tuple
import multiprocessing as mp
import time

from LocalGraphAlign import GetRegions

BLAST_IDENTITY = 90
BLAST_THREADS  = 1
OVERLAP_GAP    = 30000
EXTENDSIZE = 30000

# globals for workers
QUERYSEQ = None
OUTPUT_PATH = None
OUT_LOCK = None
LOG_LOCK = None

def reverse_complement(seq: str) -> str:
    trans_table = str.maketrans("ATCGatcg", "TAGCtagc")
    return seq.translate(trans_table)[::-1]


def sh(cmd: str, *, check: bool = False) -> None:
    subprocess.run(cmd, shell=True, check=check)


def read_fasta(path: pathlib.Path) -> Dict[str, str]:
    seqs: Dict[str, str] = {}
    with path.open(mode = 'r') as fh:
        head, buf = "", []
        for ln in fh:
            ln = ln.rstrip()
            if ln.startswith(">"):
                if head:
                    seqs[head] = "".join(buf)
                head, buf = ln[1:].split()[0], []
            else:
                buf.append(ln)
        if head:
            seqs[head] = "".join(buf)
    return seqs

def merge_intervals(spans: List[Tuple[int, int]], gap: int = OVERLAP_GAP) -> List[Tuple[int, int]]:
    """Greedy merge of (start,end) pairs allowing ≤ *gap* bp separation."""
    if not spans:
        return []
    
    spans.sort()
    merged = [list(spans[0])]
    for s, e in spans[1:]:
        last = merged[-1]
        if s - last[1] <= gap:
            last[1] = max(last[1], e)
        else:
            merged.append([s, e])
            
    return [(max(0, s - gap // 2), e + gap // 2) for s, e in merged]


def output_regions(results: List[List], output: str):
    # use global lock
    with OUT_LOCK:
        with open(output, mode="a") as w:
            for result in results:
                gindex, prefix, chrom, strd, start, end = result
                w.write(f"{gindex}\t{prefix}\t{chrom}\t{strd}\t{start}\t{end}\n")


def process_block(
    gindex: int,
    hotspot: Dict[str, List[List[int]]],
    sample: str,
    target: str,
):
    # use globals in worker
    query = QUERYSEQ
    output = OUTPUT_PATH

    prefix = target.split("/")[-1].split("_")[0]

    folder = os.path.dirname(target)
    os.makedirs(os.path.join(folder, "samples"), exist_ok=True)
    hotspot_fa = f"{folder}/samples/{sample}_hotspot.txt.fa"

    regions = cl.defaultdict(list)
    for chrom, region in hotspot.items():
        regions[chrom] = merge_intervals(region)

    allresults = []
    index = 1

    t0 = time.perf_counter()
    for contig, regionlist in regions.items():
        seq_ref = query[contig]
        for beg, end in regionlist:

            seq = seq_ref[beg:end]
            # write query fasta for this hotspot
            with open(hotspot_fa, "w") as out:
                out.write(f">{sample}_{index}\t{contig}:{beg}-{end}+\n")
                out.write(seq + "\n")
            index += 1
            lowercase = sum(1 for c in seq if c.islower())
            outfile = hotspot_fa + "_align.out"
            print(f"working on file:  {hotspot_fa}, {contig}:{beg}-{end} on {target}")
            # NOTE: adjust args to GetRegions to match your actual signature
            # I assume: GetRegions(input_fa, graph_target, out_tmp, lowercase)
            # we can write to a temp file and read back, but since your original
            # code returned results, let's keep that pattern:
            results = GetRegions(hotspot_fa, target, outfile, lowercase, 2)

            newresults = []
            for result in results:
                if result[0] == '+':
                    newresults.append([result[0], beg + result[1], beg + result[2]])
                else:
                    newresults.append([result[0], end - 1 - result[2], end - 1 - result[1]])

            allresults.extend([[gindex,prefix, contig] + x for x in newresults])

            #Path(hotspot_fa).unlink(missing_ok=True)
            #Path(hotspot_fa + "_align.out").unlink(missing_ok=True)

    t1 = time.perf_counter()

    with LOG_LOCK:
        print(f"[TimeLog:] {prefix} processing took {t1 - t0:.2f} seconds")
    if allresults:
        output_regions(allresults, output)


def cli() -> argparse.Namespace:
    p = argparse.ArgumentParser(description="Hotspot → local graph → regions")
    p.add_argument("-i", "--input",   required=True, help="hotspot txt (per locus)")
    p.add_argument("-q", "--query",   required=True, help="reference FASTA")
    p.add_argument("-g", "--graphs",  required=True, help="file listing graph targets, one per line")
    p.add_argument("-s", "--samplename",  required=True, help="sample name")
    p.add_argument("-o", "--output",  required=True, help="output BED")
    p.add_argument("-t", "--threads",   type=int, default=4, help="number of threads")
    return p.parse_args()


def main() -> None:
    global QUERYSEQ, OUTPUT_PATH, OUT_LOCK, LOG_LOCK

    args = cli()

    # 1) targets
    targets = []
    with open(args.graphs, mode = 'r') as f:
        for line in f:
            line = line.strip()
            if line:
                targets.append(line)

    # 2) reference
    QUERYSEQ = read_fasta(pathlib.Path(args.query))

    # 3) hotspots input -> build structure:
    #    hotspots[gindex][contig] = [[beg, end], ...]
    hotspots = cl.defaultdict(lambda: cl.defaultdict(list))
    with open(args.input, mode = 'r') as f:
        for line in f:
            parts = line.strip().split()
            # expected: chrom ... beg end
            # adjust field indices to your actual format
            contig = parts[0]
            beg0 = int(parts[-2])
            end0 = int(parts[-1])

            beg = max(0, beg0 - EXTENDSIZE)
            end = min(len(QUERYSEQ[contig]), end0 + EXTENDSIZE)

            gindex = int(parts[1])  # you did this in your code
            hotspots[gindex][contig].append([beg, end])

    # 4) prepare output
    OUTPUT_PATH = args.output
    open(OUTPUT_PATH, "w").close()

    # 5) shared lock
    OUT_LOCK = mp.Lock()
    LOG_LOCK = mp.Lock()
    # 6) run in parallel
    tasks = []
    for gindex, hotspot in hotspots.items():
        # gindex are 1-based, target file is 0-based in your code
        target = targets[gindex - 1]
        tasks.append((gindex, hotspot, args.samplename, target))

    #for task in tasks:
        #process_block(*task)

    with mp.Pool(processes=int(args.threads//2), initializer=_init_worker,initargs=(QUERYSEQ, OUTPUT_PATH, OUT_LOCK, LOG_LOCK)) as pool:
        pool.starmap(process_block, tasks)


def _init_worker(queryseq, output_path, lock, lock2):
    # set globals inside worker
    global QUERYSEQ, OUTPUT_PATH, OUT_LOCK, LOG_LOCK
    QUERYSEQ = queryseq
    OUTPUT_PATH = output_path
    OUT_LOCK = lock
    LOG_LOCK = lock2

if __name__ == "__main__":
    main()
